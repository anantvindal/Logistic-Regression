{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a library called Cupy. It is similar to Numpy with the added advantage of doing its computations on the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_data():\n",
    "    # Load data and use Min-Max scaling\n",
    "    (train,y_train),(test,y_test) = mnist.load_data()\n",
    "    train = train.reshape(60000,28*28)/255\n",
    "    test = test.reshape(10000,28*28)/255\n",
    "    return (train,test),(y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(data,params):\n",
    "    ans = np.exp(np.matmul(data,params[0])+params[1])\n",
    "    ans = (ans/(np.sum(ans,axis=1)).reshape(data.shape[0],1))\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neg_log_loss(labels,softmax,theta,regularisation,lamb):\n",
    "    ans = -np.mean(labels*np.log(softmax))\n",
    "    if regularisation:\n",
    "        ans = ans - lamb/(2*softmax.shape[0])*(np.linalg.norm(theta))\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdash_thet0(sftmax,labels):\n",
    "    '''derivative of theta_0 is calculated as summation of labels-Softmax'''\n",
    "    ans = np.sum(sftmax-labels,axis=0)\n",
    "    return np.array(ans)/sftmax.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdash_thet(data,labels,sftmx,theta,regularisation,lamb):\n",
    "    '''derivative of theta is calculated as (Updated_labels.T x Data).T\n",
    "       Updated labels = -(labels-Softmax)\n",
    "       This negative sign is due to Negative Log Loss'''\n",
    "    ans = np.matmul(np.transpose(data),sftmx-labels)\n",
    "    if regularisation:\n",
    "        ans = np.matmul(np.transpose(data),sftmx-labels) - lamb/(2*data.shape[0])*np.sum(theta)\n",
    "    return np.array(ans)/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(training_data,labels,thetas,epsilon,learning_rate,Type=None,epochs=0,batch_size=0,regularisation=None,lamb=0):\n",
    "    '''\n",
    "    This function will use Batch Gradient Descent and Mini-Batch Gradient Descent \n",
    "    for optimisation depending upon the choice of user.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data: array-like\n",
    "        input of shape (m,n) where m represents the number of training samples.\n",
    "    \n",
    "    labels: array-like\n",
    "        input of shape (m,1)\n",
    "        should not be one hot encoded\n",
    "    \n",
    "    thetas: list of arrays [thetas,theta_0]\n",
    "        they should be pre-initialised\n",
    "        thetas should be of the shape (n,k) where k is the number of unique labels\n",
    "        theta_0 should be of the shape (1,k)\n",
    "    \n",
    "    epsilon: float\n",
    "        error tolerance\n",
    "    \n",
    "    learning_rate: float\n",
    "    \n",
    "    Type: str input, optional\n",
    "        use 'mini' for Mini-Batch Gradient Descent\n",
    "        use None for Batch Gradient Descent\n",
    "    \n",
    "    epochs: int, optional\n",
    "        should be used if user has mentioned the type of gradient descent to be used\n",
    "        \n",
    "    batch_size: int, optional\n",
    "        size of the mini batch\n",
    "        \n",
    "    regularisation: bool, optional\n",
    "        Default None\n",
    "        set to True for l2 regularisation\n",
    "        \n",
    "    lamb: int, optional\n",
    "        a positive value which will be used as regularisation parameter\n",
    "        \n",
    "        \n",
    "    returns a tuple containing thet_f, thet0_f and neg_loss_history\n",
    "    '''\n",
    "    assert thetas[0].shape==(training_data.shape[1],len(np.unique(labels)))\n",
    "    assert thetas[1].shape==(1,len(np.unique(labels)))\n",
    "    assert epochs>=0\n",
    "    assert learning_rate>=0\n",
    "    assert epsilon>=0\n",
    "    assert lamb>=0\n",
    "    assert batch_size>=0 and batch_size<training_data.shape[0]\n",
    "    \n",
    "    \n",
    "    thet_i = thetas[0]\n",
    "    thet0_i = thetas[1]\n",
    "    \n",
    "    iterations=[]\n",
    "    neg_log_history=[]\n",
    "    deriv=[]\n",
    "    iden=np.identity(len(np.unique(labels)))\n",
    "    classes_train = labels\n",
    "        \n",
    "    if Type==None:\n",
    "        training_data_1 = training_data\n",
    "        classes_tr = np.array(iden[classes_train])\n",
    "        i=0\n",
    "        while True:\n",
    "            sftmx_ini = Softmax(training_data,[thet_i,thet0_i])\n",
    "\n",
    "            thet0_f = thet0_i - learning_rate*(fdash_thet0(sftmx_ini,classes_tr))\n",
    "            thet_f = thet_i - learning_rate*(fdash_thet(training_data_1,classes_tr,sftmx_ini,theta=thet_i,regularisation=regularisation,lamb=lamb))\n",
    "\n",
    "            sftmx_fin = Softmax(training_data_1,[thet_f,thet0_f])\n",
    "\n",
    "\n",
    "            neg_loss = abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb) - Neg_log_loss(classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "            neg_log_history.append(abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb)))\n",
    "            iterations.append(i)\n",
    "            deriv.append(thet_f[:,0])\n",
    "            if i%500==0:\n",
    "                print('neg log loss at iteration {} is {}'.format(i,neg_log_history[-1]))\n",
    "\n",
    "            i+=1\n",
    "\n",
    "            if (neg_loss)<epsilon:\n",
    "                print('neg log loss final at iteration {} is {}'.format(i,neg_log_history[-1]))\n",
    "                return thet_f,thet0_f,neg_log_history,deriv\n",
    "            else:\n",
    "                thet0_i = thet0_f\n",
    "                thet_i = thet_f\n",
    "\n",
    "                \n",
    "    elif Type=='mini':\n",
    "        \n",
    "        batches = training_data.shape[0]//batch_size\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for batch in range(batches):\n",
    "                training_data_1 = np.array(training_data[batch*batch_size:(batch+1)*batch_size])\n",
    "                classes_tr = np.array(iden[classes_train[batch*batch_size:(batch+1)*batch_size]])\n",
    "                \n",
    "                sftmx_ini = Softmax(training_data_1,[thet_i,thet0_i])\n",
    "\n",
    "                thet0_f = thet0_i - learning_rate*(fdash_thet0(sftmx_ini,classes_tr))\n",
    "                thet_f = thet_i - learning_rate*(fdash_thet(training_data_1,classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "\n",
    "                sftmx_fin = Softmax(training_data_1,[thet_f,thet0_f])\n",
    "                \n",
    "\n",
    "                neg_loss = abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb) - Neg_log_loss(classes_tr,sftmx_ini,thet_i,regularisation,lamb))\n",
    "                neg_log_history.append(abs(Neg_log_loss(classes_tr,sftmx_fin,thet_f,regularisation,lamb)))\n",
    "                iterations.append((e,batch))\n",
    "\n",
    "                thet0_i = thet0_f\n",
    "                thet_i = thet_f\n",
    "            if e%10==0:\n",
    "                print('neg log loss after epoch {} is {}'.format(e,neg_log_history[-1]))\n",
    "            deriv.append(thet_f[:,0])\n",
    "\n",
    "        print('neg log loss after epoch {} is {}'.format(e,neg_log_history[-1]))\n",
    "    return thet_f,thet0_f,neg_log_history,deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(train,test),(classes_train,classes_test) = Load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(213, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA for dimensionality reduction. This will reduce the amount of calculations to be done and will\n",
    "#improve the execution speed.\n",
    "cov_matrix = pd.DataFrame(train).cov()\n",
    "q,lam,qt = np.linalg.svd(cov_matrix)\n",
    "trace = np.sum(lam)\n",
    "f_vector=[]\n",
    "s=0\n",
    "for i in range(len(lam)):\n",
    "    s+=lam[i]\n",
    "    if s/trace>0.97:\n",
    "        break\n",
    "    else:\n",
    "        f_vector.append(q[:,i])\n",
    "f_vector = np.array(f_vector)\n",
    "f_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 213)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = np.matmul(np.array(train),np.transpose(f_vector))\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg log loss after epoch 0 is 0.1777537280064086\n",
      "neg log loss after epoch 10 is 0.043883561689200025\n",
      "neg log loss after epoch 20 is 0.02455075174261439\n",
      "neg log loss after epoch 30 is 0.016727286195589004\n",
      "neg log loss after epoch 40 is 0.012331537599606228\n",
      "neg log loss after epoch 50 is 0.009437511998576582\n",
      "neg log loss after epoch 60 is 0.007345367307110983\n",
      "neg log loss after epoch 70 is 0.00573663757703362\n",
      "neg log loss after epoch 80 is 0.004444410101457651\n",
      "neg log loss after epoch 90 is 0.003372239855612042\n",
      "neg log loss after epoch 99 is 0.002545480956634273\n"
     ]
    }
   ],
   "source": [
    "#Run the training process\n",
    "thet0_i = np.zeros(shape=(1,len(np.unique(classes_train))))\n",
    "thet_i = np.zeros(shape=(training_data.shape[1],len(np.unique(classes_train))))\n",
    "\n",
    "tf,t0f,nlh,derivs = gradient_descent(training_data=training_data,labels=classes_train,thetas=[thet_i,thet0_i],\n",
    "                                     epsilon=1e-6,learning_rate=0.001,Type='mini',epochs=100,batch_size=128,\n",
    "                                     regularisation=True,lamb=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931166666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training accuracy\n",
    "ans=[]\n",
    "sft=Softmax(training_data,[tf,t0f])\n",
    "for i in range(training_data.shape[0]):\n",
    "    ans.append(np.argmax(sft[i]))\n",
    "np.count_nonzero(ans==classes_train)/training_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "iden = np.identity(len(np.unique(classes_test)))\n",
    "classes_te = np.array(iden[classes_test])\n",
    "test = np.matmul(test,np.transpose(f_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing accuracy\n",
    "test_sft=Softmax(test,[tf,t0f])\n",
    "ans_test = []\n",
    "for i in range(test.shape[0]):\n",
    "    ans_test.append(np.argmax(test_sft[i]))\n",
    "np.count_nonzero(ans_test==classes_test)/test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy can be improved by tuning our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
